# Statistical Tests and Models

## An Overview of Statistical Testing in Python

### Introduction

This section was written by John Ackerman, a sophomore at the University of 
Connecticut double majoring in mathematics and statistical data science.

By the end of this section, you'll understand how to approach problems of the 
following forms using Python:  

- Does the average height of male and female university students differ 
significantly?  
- Is there a relationship between the type of movie a person sees and 
whether or not they buy snacks at the theater?  
- Can we predict whether a patient has a disease based on their temperature 
and gender?  
- How can a company estimate customer satisfaction with a small sample of 
feedback?  

### What's Statistical Testing?

Statistical testing is a method of making probabilistic decisions about a 
population parameter based on sample data. It involves formulating hypotheses 
and using data to assess the evidence against a null hypothesis.

### Null and Alternate Hypotheses

The null hypothesis ($H_o$) is the default assumption that there is no effect, 
no difference, or no relationship in the data. The alternative hypothesis 
($H_1$ or $H_a$) is what we seek to provide evidence for-it suggests that 
there is an effect, a difference, or a relationship. Hypothesis testing 
evaluates whether the observed data provides enough evidence to reject the 
null hypothesis in favor of the alternative.

The following table contains the null and alternative hypotheses for some 
common tests:

| Test | Null Hypothesis (H₀) | Alternative Hypothesis (H₁ or Hₐ) |
|------|----------------------|-----------------------------------|
| t-test (one-sample or two-sample) | The population mean(s) are equal. | The population mean(s) are different. |
| Chi-square test for independence | Two categorical variables are independent. | Two categorical variables are dependent. |
| ANOVA (Analysis of Variance) | All group means are equal. | At least one group mean is different. |
| Linear Regression (F-test) | The independent variable(s) have no effect on the dependent variable. | At least one independent variable significantly affects the dependent variable. |
| Mann-Whitney U test | The distributions of the two groups are the same. | The distributions of the two groups are different. |

### Assumptions

Statistical tests are based on certain assumptions about the data, such as the 
distribution, variance, or independence of observations. Violating assumptions 
increases your risk of Type I errors (falsely rejecting a true null 
hypothesis) and Type II errors (failing to reject a false null hypothesis).

| Assumption | Method to Test | Package:Function |
|------------|----------------|------------------|
| Normality: (data is normally distributed) | Shapiro-Wilk test or Q-Q plot | `scipy.stats.shapiro` (Shapiro-Wilk Test), `statsmodels.api.qqplot` (Q-Q Plot) |
| Independence: (observations are independent) | Durbin-Watson test (for time series) | `statsmodels.stats.stattools.durbin_watson` (Durbin-Watson Test) |
| Homoscedasticity: (equal variance across groups) | Levene’s test | `scipy.stats.levene` (Levene’s Test) |
| Heteroscedasticity: (unequal variance) | Breusch-Pagan test | `statsmodels.stats.diagnostic.het_breuschpagan` (Breusch-Pagan Test) |
| No Outliers: (no extreme values) | IQR method or Z-score | `scipy.stats.iqr` (Interquartile Range), `scipy.stats.zscore` (Z-score) |
| No Multicollinearity: (independent predictors in regression) | Variance Inflation Factor (VIF) | `statsmodels.stats.outliers_influence.variance_inflation_factor` (VIF) |

### Parametric vs. Non-Parametric

Parametric tests assume that the data follows a specific distribution 
(typically normal), and rely on parameters such as the mean and standard 
deviation. Non-parametric tests don't assume any distribution and as such 
are used when parametric assumptions are violated. Both are important because 
parametric tests offer greater precision if assumptions hold, while 
non-parametric tests provide a robust alternative when they don't.

Let's run a series of simulations to compare the accuracy of the independent 
t-test (parametric) vs the Wilcoxon test (non-parametric) with right-skewed 
samples with a known difference between groups:

```{python}
import numpy as np
import matplotlib.pyplot as plt
from scipy import stats

# Set random seed
np.random.seed(124)

# Simulation parameters
n_simulations = 1000
sample_size = 30

# Arrays to store p-values
t_test_pvalues = []
wilcoxon_pvalues = []

# Run simulations
for _ in range(n_simulations):
    # Generate skewed data with a true difference
    group1 = np.random.lognormal(mean=0, sigma=0.8, size=sample_size)
    group2 = np.random.lognormal(mean=0.3, sigma=0.8, size=sample_size)  
    
    # Run both tests
    _, t_pvalue = stats.ttest_ind(group1, group2)
    _, w_pvalue = stats.mannwhitneyu(group1, group2)
    
    # Storing p-values
    t_test_pvalues.append(t_pvalue)
    wilcoxon_pvalues.append(w_pvalue)

# Find prop of results w/ (p < 0.05)
t_test_significant = (
  np.sum(np.array(t_test_pvalues) < 0.05) / n_simulations
)
wilcoxon_significant = (
  np.sum(np.array(wilcoxon_pvalues) < 0.05) / n_simulations
)

# Plot histograms
plt.figure(figsize=(8, 5))

plt.subplot(1, 2, 1)
plt.hist(t_test_pvalues, bins=20, color='blue')
plt.axvline(x=0.05, color='red', linestyle='--')
plt.ylim(None, 310)
plt.title(f't-test\nDetection Rate: {t_test_significant:.2f}')
plt.xlabel('p-value')
plt.ylabel('Frequency')

plt.subplot(1, 2, 2)
plt.hist(wilcoxon_pvalues, bins=20, color='green')
plt.axvline(x=0.05, color='red', linestyle='--')
plt.ylim(None, 310)
plt.title(f'Wilcoxon Test\nDetection Rate: {wilcoxon_significant:.2f}')
plt.xlabel('p-value')

plt.tight_layout()
plt.show()

print(
  f"Proportion of significant t-test results: "
  f"{t_test_significant:.2f}"
)
print(
  f"Proportion of significant Wilcoxon results: "
  f"{wilcoxon_significant:.2f}"
)
print(
  f"The non-parametric Wilcoxon test correctly identifies the difference "
  f"{wilcoxon_significant/t_test_significant:.2f}x more often"
)
```

The Wilcoxon test correctly detected the difference more often because it 
didn't assume the samples followed a normal distribution.

### Types of Statistical Testing

#### Is There a Significant Difference? (A/B Testing)

These tests help determine whether the observed differences between groups are 
significant or just due to chance.

Ex: Does our new drug significantly lower blood pressure compared to placebo?

##### Overview of Relevant Tests:

| Test | Use Case | Key Assumptions | Package:Function |
|------|----------|-----------------|------------------|
| t-test | Compare two means | Normality, equal variance (for independent test) | `scipy.stats:ttest_ind` |
| Mann-Whitney U | Compare two groups (non-parametric) | Continuous data, different distributions | `scipy.stats:mannwhitneyu` |
| ANOVA | Compare 3+ groups | Normality, equal variance | `scipy.stats:f_oneway` |
| Kruskal-Wallis | Compare 3+ groups (non-parametric) | Same shape distributions | `scipy.stats:kruskal` |

##### t-test & Mann-Whitney U Python Implementation:

```{python}
from scipy.stats import ttest_ind, mannwhitneyu

group_a = [5, 7, 8, 6, 9]
group_b = [6, 8, 7, 10, 12]

# If data is normally distributed:
t_stat, p_value = ttest_ind(group_a, group_b)
print(f"T-test: T={t_stat:.3f}, p={p_value:.3f}")

# If data is non-normal:
u_stat, p_value = mannwhitneyu(group_a, group_b)
print(f"Mann-Whitney U: U={u_stat:.3f}, p={p_value:.3f}")
```

##### Key Points:

When choosing statistical tests, follow these guidelines:  

- Use a t-test if the data is normal; otherwise, use the Mann-Whitney U test.  
- For three or more groups, use ANOVA (or Kruskal-Wallis if non-parametric).  
- For paired and one-sample t-tests, use SciPy's `ttest_rel` and `ttest_1samp`.  

#### Are Two Variables Related? 

These tests assess whether two variables are correlated or if their 
relationship is just due to random variation.

Ex: Is there a correlation between hours studied and test performance?

##### Overview of Relevant Tests:

| Test | Use Case | Key Assumptions | Package:Function |
|------|----------|-----------------|------------------|
| Pearson correlation | Linear relationship between variables | Both variables are continuous and normally distributed | `scipy.stats:pearsonr` |
| Spearman correlation | Monotonic relationship between variables | No normality assumption | `scipy.stats:spearmanr` |
| Chi-square test | Relationship between two categorical variables | Expected frequency >5 in each category | `scipy.stats:chi2_contingency` |

##### Pearson & Spearman Python Implementation

```{python}
from scipy.stats import pearsonr, spearmanr

x = [10, 20, 30, 40, 50]
y = [15, 25, 35, 45, 60]

# Pearson correlation
corr, p_value = pearsonr(x, y)
print(f"Pearson correlation: {corr:.3f}, p={p_value:.3f}")

# Spearman correlation (if data is non-normal)
corr, p_value = spearmanr(x, y)
print(f"Spearman correlation: {corr:.3f}, p={p_value:.3f}")
```

##### Key Points

For correlation analysis:  

- Use Pearson’s correlation for linear relationships and Spearman’s 
correlation for monotonic ones.  
- Use the chi-square test when dealing with categorical data.  
- Remember that correlation $\neq$ causation.  

#### Can We Predict One Variable from Another?

These methods reveal how well one variable can predict another and quantify 
the strength of that relationship.

Ex: Can we predict a car's fuel efficiency with its weight and horsepower?

##### Overview of Relevant Tests:

| Test | Use Case | Key Assumptions | Package:Function |
|------|----------|-----------------|------------------|
| Linear Regression | Predict a continuous outcome | Linearity, normal residuals | `sklearn.linear_model:LinearRegression` |
| Logistic Regression | Predict a binary outcome | Linear relationship between predictors and log-odds | `sklearn.linear_model:LogisticRegression` |
| Chi-square goodness-of-fit | Compare observed vs. expected frequencies | Categories should have expected counts >5 | `scipy.stats:chisquare` |

##### Linear Regression Python Implementation:

```{python}
from sklearn.linear_model import LinearRegression
import numpy as np

# Sample data
X = np.array([1, 2, 3, 4, 5]).reshape(-1, 1)
y = np.array([2, 4, 5, 4, 5])

# Fit a simple linear regression model
model = LinearRegression()
model.fit(X, y)
predictions = model.predict(X)

print(f"Predicted values: {predictions}")
```

##### Key Points:

For regression modeling:  

- Use linear regression for continuous outcomes and logistic regression for 
binary outcomes.  
- Consider feature scaling and transformations when using regression models.  

#### Is This Result Significant or Noise? (Alternative Inference)

These methods help distinguish meaningful patterns from random noise while 
providing estimates of the existence and magnitude of effects. They offer 
robust alternatives to traditional approaches by using simulation and 
resampling techniques, making them valuable when standard assumptions are 
violated.

Ex: Amazon is considering a new design for their checkout page and wants to 
know if the new design increases conversion rate and by how much.

##### Overview of Relevant Tests

| Method | Use Case | Key Idea | Package:Function |
|--------|----------|----------|------------------|
| Bootstrap CI | Estimate confidence intervals from data | Resample data many times to see possible variations | `numpy.random:choice` (custom implementation) |
| Permutation Test | Test if two groups are meaningfully different | Shuffle labels and see how extreme the observed difference is | `scipy.stats:permutation_test` |
| Bayesian Inference | Quantify belief in different hypotheses | Treat parameters as probabilities and update based on data | `pymc3` / `scipy.stats:beta` (for simple cases) |

##### Python Implementation of all methods:

```{python}
import numpy as np
from scipy.stats import permutation_test, beta

# Simulated data: time spent on old vs. new layout
old_layout = np.array([40, 50, 45, 55, 42, 48])
new_layout = np.array([52, 60, 58, 65, 55, 57])

# Bootstrap CI
bootstrap_samples = np.random.choice(
  new_layout - old_layout,
  size=(10000, len(old_layout)),
  replace=True
)
ci_lower, ci_upper = np.percentile(
  bootstrap_samples.mean(axis=1), [2.5, 97.5]
)
print(f"Bootstrap 95% CI: [{ci_lower:.3f}, {ci_upper:.3f}]")

# Permutation test
perm_result = permutation_test(
  (old_layout, new_layout),
  statistic=lambda x, y: np.mean(x) - np.mean(y),
  n_resamples=10000
)
print(f"Permutation test p-value: {perm_result.pvalue:.3f}")

# Bayesian inference estimates probability that new layout increases time spent
alpha, beta_params = 1 + new_layout.sum(), 1 + old_layout.sum()
bayes_prob = 1 - beta.cdf(0, alpha, beta_params)
print(f"Bayesian probability that new layout is better: {bayes_prob:.3f}")
```

##### Key Points:

Principle ideas for the three methods:

- Focus on effect sizes and uncertainty rather than just p-values.  
- Effect sizes often provide more actionable insights.  
- Bootstrap confidence intervals provide robust interval estimates without 
normality assumptions.  
- Permutation tests allow inference without relying on parametric models.  
- Bayesian methods offer probability-based conclusions rather than binary 
p-values.  

### Statistical vs. Practical Significance

#### Multiple Testing

Multiple testing refers to performing several statistical tests on the same 
data or research question simultaneously, which increases the risk of false 
positives (Type I errors). For example, suppose the null hypothesis is true, 
and we use a significance level 0.05. With a single test, there's a 5% chance 
of a Type I error. However, if we conduct 10 tests, the probability of 
encountering at least one Type I error rises to approximately 40%.

A common corrective measure is the Bonferroni correction, which adjusts the 
significance level as follows:  

$$
\alpha_{\text{adjusted}} = \frac{\alpha}{m},
$$

where $\alpha$ is the significance level and $m$ is the number of analyses.  


#### Effect Size

P-values only inform us whether an effect exists.
Consider that large sample sizes can make small effects significant, and small 
samples might overlook large effects. Effect size quantifies the magnitude of 
a difference or relationship, providing context beyond statistical 
significance. One such measure for mean difference is Cohen's d, which is 
defined as:  

$$
d = \frac{\bar{X}_1 - \bar{X}_2}{s_p},
$$

where $\bar{X}_1$ and $\bar{X}_2$ are the sample means, and $s_p$ is the 
pooled standard deviation. The pooled standard deviation is calculated as:  

$$
s_p = \sqrt{\frac{(n_1 - 1)s_1^2 + (n_2 - 1)s_2^2}{n_1 + n_2 - 2}},
$$

where $s_1$ and $s_2$ are the standard deviations, and $n_1$ and $n_2$ are the 
sample sizes. 

Other measures include:  

- Correlation Coefficient: Measures how two continuous variables relate.  
- Eta-Squared: Used for ANOVA; quantifies the proportion of variance explained 
by a categorical factor.  
- Phi and Cramer’s V: Used for categorical data in chi-square tests.

### Best Practices  

#### Assumption Violations
- Check assumptions before choosing a test rather than after obtaining results.  
- Use non-parametric alternatives when assumptions are violated and connot 
be easily corrected.  
- Try transforming the data to better meet assumptions.

#### Interpretation Guidelines  
- Use p-values alongside effect sizes and confidence intervals for a more 
complete analysis.
- Be catious of multiple comparisons and adjust significance levels when 
necessary.

#### Documentation  
- Ensure code and data preprocessing steps are well-documented for 
reproducibility.
- Report all relevant statistics.  
- Include visualizations when possible. 

### Further Reading
- [scipy.stats](https://docs.scipy.org/doc/scipy/reference/stats.html)
- [statsmodels.stats](https://www.statsmodels.org/stable/stats.html)
- [sklearn.userguide](https://scikit-learn.org/stable/model_selection.html)

## Tests for Exploratory Data Analysis

A collection of functions are available from `scipy.stats`.

+ Comparing the locations of two samples
    - `ttest_ind`: t-test for two independent samples
    - `ttest_rel`: t-test for paired samples
	- `ranksums`: Wilcoxon rank-sum test for two independent samples
	- `wilcoxon`: Wilcoxon signed-rank test for paired samples
+ Comparing the locations of multiple samples
    - `f_oneway`: one-way ANOVA
	- `kruskal`: Kruskal-Wallis H-test
+ Tests for associations in contigency tables
    - `chi2_contingency`: Chi-square test of independence of variables
	- `fisher_exact`:  Fisher exact test on a 2x2 contingency table
+ Goodness of fit
    - `goodness_of_fit`: distribution could contain unspecified parameters
	- `anderson`: Anderson-Darling test 
    - `kstest`: Kolmogorov-Smirnov test 
	- `chisquare`: one-way chi-square test
	- `normaltest`: test for normality


Since R has a richer collections of statistical functions, we can call 
R function from Python with `rpy2`. See, for example, a [blog on this
subject](https://rviews.rstudio.com/2022/05/25/calling-r-from-python-with-rpy2/).


For example, `fisher_exact` can only handle 2x2 contingency tables. For
contingency tables larger than 2x2, we can call `fisher.test()` from R through
`rpy2`. 
See [this StackOverflow post](https://stackoverflow.com/questions/25368284/fishers-exact-test-for-bigger-than-2-by-2-contingency-table).
Note that the `.` in function names and arguments are replaced with `_`.

```{python}
import pandas as pd
import numpy as np
import rpy2.robjects.numpy2ri
from rpy2.robjects.packages import importr
rpy2.robjects.numpy2ri.activate()

stats = importr('stats')

w0630 = pd.read_feather("data/nyccrashes_cleaned.feather")
w0630["injury"] = np.where(w0630["number_of_persons_injured"] > 0, 1, 0)
m = pd.crosstab(w0630["injury"], w0630["borough"])
print(m)

res = stats.fisher_test(m.to_numpy(), simulate_p_value = True)
print(res)
```



## Statistical Modeling

Statistical modeling is a cornerstone of data science, offering tools to
understand complex relationships within data and to make predictions. Python,
with its rich ecosystem for data analysis, features the `statsmodels` package—
a comprehensive library designed for statistical modeling, tests, and data
exploration. `statsmodels` stands out for its focus on classical statistical
models and compatibility with the Python scientific stack (`numpy`, `scipy`,
`pandas`).

### Installation of `statsmodels`

To start with statistical modeling, ensure `statsmodels` is installed:

Using pip:

```bash
pip install statsmodels
```

### Linear Model

Let's simulate some data for illustrations.

```{python}
import numpy as np

nobs = 200
ncov = 5
np.random.seed(123)
x = np.random.random((nobs, ncov)) # Uniform over [0, 1)
beta = np.repeat(1, ncov)
y = 2 + np.dot(x, beta) + np.random.normal(size = nobs)
```

Check the shape of `y`:
```{python}
y.shape
```

Check the shape of `x`:
```{python}
x.shape
```

That is, the true linear regression model is
$$
y = 2 + x_1 + x_2 + x_3 + x_4 + x_5 + \epsilon.
$$

A regression model for the observed data can be fitted as

```{python}
import statsmodels.api as sma
xmat = sma.add_constant(x)
mymod = sma.OLS(y, xmat)
myfit = mymod.fit()
myfit.summary()
```

Questions to review:

+ How are the regression coefficients interpreted? Intercept?
+ Why does it make sense to center the covariates?


Now we form a data frame with the variables

```{python}
import pandas as pd
df = np.concatenate((y.reshape((nobs, 1)), x), axis = 1)
df = pd.DataFrame(data = df,
                  columns = ["y"] + ["x" + str(i) for i in range(1,
                  ncov + 1)])
df.info()
```

Let's use a formula to specify the regression model as in R, and fit
a robust linear model (`rlm`) instead of OLS. Note that the model specification
and the function interface is similar to R.

```{python}
import statsmodels.formula.api as smf
mymod = smf.rlm(formula = "y ~ x1 + x2 + x3 + x4 + x5", data = df)
myfit = mymod.fit()
myfit.summary()
```

For model diagnostics, one can check residual plots.

```{python}
import matplotlib.pyplot as plt

myOlsFit = smf.ols(formula = "y ~ x1 + x2 + x3 + x4 + x5", data = df).fit()
fig = plt.figure(figsize = (6, 6))
## residual versus x1; can do the same for other covariates
fig = sma.graphics.plot_regress_exog(myOlsFit, 'x1', fig=fig)
```

See more on [residual diagnostics and specification
tests](https://www.statsmodels.org/stable/stats.html#residual-diagnostics-and-specification-tests).


### Generalized Linear Regression

A linear regression model cannot be applied to presence/absence or
count data.  Generalized Linear Models (GLM) extend the classical
linear regression to accommodate such response variables, that follow
distributions other than the normal distribution. GLMs consist of
three main components:


+ Random Component: This specifies the distribution of the 
response variable $Y$. It is assumed to be from the exponential family of 
distributions, such as Binomial for binary data and Poisson for count data.
+ Systematic Component: This consists of the linear predictor, 
a linear combination of unknown parameters and explanatory variables. It 
is denoted as $\eta = X\beta$, where $X$ represents the explanatory 
variables, and $\beta$ represents the coefficients.
+ Link Function: The link function, $g$, provides the 
relationship between the linear predictor and the mean of the distribution 
function. For a GLM, the mean of $Y$ is related to the linear predictor 
through the link function as $\mu = g^{-1}(\eta)$.


GLMs adapt to various data types through the
selection of appropriate link functions and probability distributions. Here,
we outline four special cases of GLM: normal regression, logistic regression,
Poisson regression, and gamma regression.


+ Normal Regression (Linear Regression).
In normal regression, the response variable has a normal distribution. The
identity link function is typically used, making this case
equivalent to classical linear regression.
    - Use Case: Modeling continuous data where residuals are normally distributed.
    - Link Function: Identity, $g(\mu) = \mu$.
    - Distribution: Normal.

+ Logistic Regression.
Logistic regression is used for binary response variables. It employs the
logit link function to model the probability that an observation falls into
one of two categories.
    - Use Case: Binary outcomes (e.g., success/failure).
    - Link Function: Logit, $g(\mu) = \log\frac{\mu}{1-\mu}$.
    - Distribution: Binomial.

+ Poisson Regression.
Poisson regression models count data using the Poisson distribution. It's
ideal for modeling the rate at which events occur.
    - Use Case: Count data, such as the number of occurrences of an event.
    - Link Function: Log, $g(\mu) = \log(\mu)$
    - Distribution: Poisson.

+ Gamma Regression.
Gamma regression is suited for modeling positive continuous variables, 
especially when data are skewed and variance increases with the mean.
    - Use Case: Positive continuous outcomes with non-constant variance.
    - Link Function: Inverse $g(\mu) = \frac{1}{\mu}$.
    - Distribution: Gamma.

Each GLM variant addresses specific types of data and research questions,
enabling precise modeling and inference based on the underlying data
distribution. Prediction will need the inverse link function which
transforms the linear predictor to the expectation of the outcome.

To demonstrate the validation of logistic regression models, we first
create a simulated dataset with binary outcomes. This setup involves
generating logistic probabilities and then drawing binary outcomes
based on these probabilities.

```{python}
import numpy as np
import pandas as pd
import statsmodels.api as sm

# Set seed for reproducibility
np.random.seed(42)

# Create a DataFrame with random features named `simdat`
simdat = pd.DataFrame(np.random.randn(1000, 5), columns=['x1', 'x2', 'x3', 'x4', 'x5'])

# Calculating the linear combination of inputs plus an intercept
eta = simdat.dot([2, 2, 2, 0, 0]) - 5

# Applying the logistic function to get probabilities using statsmodels' logit link
p = sm.families.links.Logit().inverse(eta)

# Generating binary outcomes based on these probabilities and adding them to `simdat`
simdat['yb'] = np.random.binomial(1, p, p.size)

# Display the first few rows of the dataframe
print(simdat.head())
```

Fit a logistic regression for `y1b` with the formula interface.

```{python}
import statsmodels.formula.api as smf

# Specify the model formula
formula = 'yb ~ x1 + x2 + x3 + x4 + x5'

# Fit the logistic regression model using glm and a formula
fit = smf.glm(formula=formula, data=simdat, family=sm.families.Binomial()).fit()

# Print the summary of the model
print(fit.summary())
```


## Logistic Regression

Once a logistic regression model is fitted, interpreting its results
is crucial for understanding how predictor variables influence the
probability of the outcome. Logistic regression models the log-odds of
the response variable as a linear function of the predictor
variables. To ease the intrepretation, consider a logistic model with
a single binary predictor (e.g., treatment indicator):

$$
\log\left(\frac{\mu}{1 - \mu}\right) = \beta_0 + \beta_1 X
$$

where $\mu = E(Y \mid X)$ represents the probability of the positive
class, and $\beta_1$ is the estimated coefficient for the binary
predictor $X$.


### Interpreting Coefficients

If $X$ is a binary variable (e.g., 0 for "No" and 1 for "Yes"),
$\beta_1$ represents the difference in log-odds between the two
groups. Exponentiating $\beta_1$ gives the odds ratio:

$$
\text{Odds Ratio} = \frac{\exp(\beta_0 + \beta_1)}{\exp(\beta_0)} = e^{\beta_1}.
$$

- If $e^{\beta_1} > 1$, the outcome is more likely when $X = 1$ than when $X = 0$.
- If $e^{\beta_1} < 1$, the outcome is less likely when $X = 1$.
- If $e^{\beta_1} = 1$, there is no effect of $X$ on the odds of the outcome.

Equivalently, $\beta_1$ is the log odds ratio between the two groups.

When there are multiple predictors, the intrepretation needs to state
that all the other predictors are unchanged.

How would you intreprete the coefficient of a continuous predictor?

### Probabilistic Interpretation

We can transform the linear predictor into a probability estimate
using the inverse logit function:

$$
\Pr(Y=1 | X) = \frac{e^{\beta_0 + \beta_1 X}}{1 + e^{\beta_0 + \beta_1 X}}.
$$

This allows for a direct interpretation of how being in one category
of $X$ influences the predicted probability of the outcome. By
construction, this value is always in $(0, 1)$.

### Evaluating Statistical Significance

The significance of $\beta_1$ is assessed using standard errors and
p-values:

- A small p-value (e.g., < 0.05) suggests that $X$ has a statistically
  significant effect on the outcome.
- Confidence intervals for $e^{\beta_1}$ help understand the precision
  of odds ratio estimates.

### Confusion Matrix
Validating the performance of logistic regression models is crucial to
assess their effectiveness and reliability. This section explores key
metrics used to evaluate the performance of logistic regression
models, starting with the confusion matrix, then moving on to
accuracy, precision, recall, F1 score, and the area under the ROC
curve (AUC). Using simulated data, we will demonstrate how to
calculate and interpret these metrics using Python.
 
 
The [confusion matrix](https://en.wikipedia.org/wiki/Confusion_matrix)
is a fundamental tool used for calculating
several other classification metrics. It is a table used to describe
the performance of a classification model on a set of data for which
the true values are known. The matrix displays the actual values
against the predicted values, providing insight into the number of
correct and incorrect predictions.

   Actual         | Predicted Positive | Predicted Negative 
 ---------- | --------------- | ---------------
  Actual Positive   | True Positive (TP)     | False Negative (FN) 
  Actual Negative  | False Positive (FP)    | True Negative (TN)


Four entries in the confusion matrix:

+ True Positive (TP): The cases in which the model correctly predicted
  the positive class.
+ False Positive (FP): The cases in which the model incorrectly
  predicted the positive class (i.e., the model predicted positive,
  but the actual class was negative).
+ True Negative (TN): The cases in which the model correctly predicted
  the negative class.
+ False Negative (FN): The cases in which the model incorrectly
  predicted the negative class (i.e., the model predicted negative,
  but the actual class was positive).
  

Four rates from the confusion matrix with actual (row) margins:

+ True positive rate (TPR): TP / (TP + FN). Also known as sensitivity.
+ False negative rate (FNR): FN / (TP + FN). Also known as miss rate.
+ False positive rate (FPR): FP / (FP + TN). Also known as false alarm, fall-out.
+ True negative rate (TNR): TN / (FP + TN). Also known as specificity.

Note that TPR and FPR do not add up to one. Neither do FNR and FPR.

+ Positive predictive value (PPV): TP / (TP + FP). Also known as precision.
+ False discovery rate (FDR): FP / (TP + FP).
+ False omission rate (FOR): FN / (FN + TN).
+ Negative predictive value (NPV): TN / (FN + TN).

Note that PPV and NP do not add up to one.

### Accuracy
Accuracy measures the overall correctness of the model and is
defined as the ratio of correct predictions (both positive and
negative) to the total number of cases examined.
```
  Accuracy = (TP + TN) / (TP + TN + FP + FN)
```

+ Imbalanced Classes: Accuracy can be misleading if there is a
  significant imbalance between the classes. For instance, in a
  dataset where 95% of the samples are of one class, a model that
  naively predicts the majority class for all instances will still
  achieve 95% accuracy, which does not reflect true predictive
  performance.
+ Misleading Interpretations: High overall accuracy might hide the
  fact that the model is performing poorly on a smaller, yet
  important, segment of the data.
  

### Precision
Precision (or PPV) measures the accuracy of positive
predictions. It quantifies the number of correct positive
predictions made.
```
  Precision = TP / (TP + FP)
```


+ Neglect of False Negatives: Precision focuses solely on the positive
  class predictions. It does not take into account false negatives
  (instances where the actual class is positive but predicted as
  negative). This can be problematic in cases like disease screening
  where missing a positive case (disease present) could be dangerous.
+ Not a Standalone Metric: High precision alone does not indicate good
  model performance, especially if recall is low. This situation could
  mean the model is too conservative in predicting positives, thus
  missing out on a significant number of true positive instances.

### Recall
Recall (Sensitivity or TPR) measures the ability of a model to
find all relevant cases (all actual positives).
```
  Recall = TP / (TP + FN)
```


+ Neglect of False Positives: Recall does not consider false positives
  (instances where the actual class is negative but predicted as
  positive). High recall can be achieved at the expense of precision,
  leading to a large number of false positives which can be costly or
  undesirable in certain contexts, such as in spam detection.
+ Trade-off with Precision: Often, increasing recall decreases
  precision. This trade-off needs to be managed carefully, especially
  in contexts where both false positives and false negatives carry
  significant costs or risks.
  

### F-beta Score
The F-beta score is a weighted harmonic mean of precision and recall,
taking into account a $\beta$ parameter such that recall is considered
$\beta$ times as important as precision:
$$
(1 + \beta^2) \frac{\text{precision} \cdot \text{recall}}
{\beta^2 \text{precision} + \text{recall}}.
$$

See [stackexchange
  post](https://stats.stackexchange.com/questions/221997/why-f-beta-score-define-beta-like-that)
  for the motivation of $\beta^2$ instead of just $\beta$.

The F-beta score reaches its best value
at 1 (perfect precision and recall) and worst at 0. 


If reducing false negatives is more important (as might be the case in
medical diagnostics where missing a positive diagnosis could be
critical), you might choose a beta value greater than 1. If reducing
false positives is more important (as in spam detection, where
incorrectly classifying an email as spam could be inconvenient), a
beta value less than 1 might be appropriate.

The F1 Score is a specific case of the F-beta score where beta is 1,
giving equal weight to precision and recall. It is the harmonic mean
of Precision and Recall and is a useful measure when you seek a
balance between Precision and Recall and there is an uneven class
distribution (large number of actual negatives).


### Receiver Operating Characteristic (ROC) Curve

The Receiver Operating Characteristic (ROC) curve is a plot that
illustrates the diagnostic ability of a binary classifier as its
discrimination threshold is varied. It shows the trade-off between the
TPR and FPR. The ROC plots TPR against FPR as the decision threshold
is varied. It can be particularly useful in evaluating the
performance of classifiers when the class distribution is imbalanced,


+ Increasing from $(0, 0)$ to $(1, 1)$.
+ Best classification passes $(0, 1)$.
+ Classification by random guess gives the 45-degree line.
+ Area between the ROC and the 45-degree line is the Gini coefficient, a measure
  of inequality.
+ Area under the curve (AUC) of ROC thus provides an important metric of
classification results.


The Area Under the ROC Curve (AUC) is a scalar value that
summarizes the performance of a classifier. It measures the total area
underneath the ROC curve, providing a single metric to compare
models. The value of AUC ranges from 0 to 1:

- AUC = 1: A perfect classifier, which perfectly separates positive and negative classes.
- AUC = 0.5: A classifier that performs no better than random chance.
- AUC < 0.5: A classifier performing worse than random.


The AUC value provides insight into the model's ability to
discriminate between positive and negative classes across all possible
threshold values.


### Demonstration

Let's apply these metrics to the `simdat` dataset to understand their
practical implications. We will fit a logistic regression model, make
predictions, and then compute accuracy, precision, and recall.


```{python}
import numpy as np
from sklearn.linear_model import LogisticRegression
from sklearn.model_selection import train_test_split
from sklearn.metrics import (
    accuracy_score, precision_score, recall_score, confusion_matrix,
    f1_score, roc_curve, auc
)
import matplotlib.pyplot as plt
from sklearn.datasets import make_classification

# Generate synthetic data
X, y = make_classification(n_samples=1000, n_features=20, random_state=42)

# Split the dataset into training and testing sets
X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.25, random_state=42)

# Fit the logistic regression model
model = LogisticRegression()
model.fit(X_train, y_train)

# Predict labels on the test set
y_pred = model.predict(X_test)

# Get predicted probabilities for ROC curve and AUC
y_scores = model.predict_proba(X_test)[:, 1]  # Probability for the positive class

# Compute confusion matrix
cm = confusion_matrix(y_test, y_pred)

# Calculate accuracy, precision, and recall
accuracy = accuracy_score(y_test, y_pred)
precision = precision_score(y_test, y_pred)
recall = recall_score(y_test, y_pred)

# Print confusion matrix and metrics
print("Confusion Matrix:\n", cm)
print(f"Accuracy: {accuracy:.2f}")
print(f"Precision: {precision:.2f}")
print(f"Recall: {recall:.2f}")
```


By varying threshold, one can plot the whole ROC curve.

```{python}
# Compute ROC curve and AUC
fpr, tpr, thresholds = roc_curve(y_test, y_scores)
roc_auc = auc(fpr, tpr)

# Print AUC
print(f"AUC: {roc_auc:.2f}")

# Plot ROC curve
plt.figure()
plt.plot(fpr, tpr, color='darkorange', lw=2, label=f'ROC curve (AUC = {roc_auc:.2f})')
plt.plot([0, 1], [0, 1], color='navy', lw=2, linestyle='--')  # Diagonal line (random classifier)
plt.xlim([0.0, 1.0])
plt.ylim([0.0, 1.05])
plt.xlabel('False Positive Rate')
plt.ylabel('True Positive Rate')
plt.title('Receiver Operating Characteristic (ROC) Curve')
plt.legend(loc="lower right")
plt.show()
```


We could pick the best threshold that optmizes F1-score/
```{python}
# Compute F1 score for each threshold
f1_scores = []
for thresh in thresholds:
    y_pred_thresh = (y_scores >= thresh).astype(int)  # Apply threshold to get binary predictions
    f1 = f1_score(y_test, y_pred_thresh)
    f1_scores.append(f1)

# Find the best threshold (the one that maximizes F1 score)
best_thresh = thresholds[np.argmax(f1_scores)]
best_f1 = max(f1_scores)

# Print the best threshold and corresponding F1 score
print(f"Best threshold: {best_thresh:.4f}")
print(f"Best F1 score: {best_f1:.2f}")
```


## LASSO Logistic Models

The Least Absolute Shrinkage and Selection Operator (LASSO)
[@tibshirani1996regression], is a regression method that
performs both variable selection and regularization. LASSO
imposes an L1 penalty on the regression coefficients, which has the
effect of shrinking some coefficients exactly to zero. This results in
simpler, more interpretable models, especially in situations where the
number of predictors exceeds the number of observations.


### Theoretical Formulation of the Problem


The objective function for LASSO logistic regression can be expressed
as,

$$
\min_{\beta} 
\left\{ -\frac{1}{n} \sum_{i=1}^n \left[ y_i \log(\hat{p}_i) + (1 - y_i) \log(1 - \hat{p}_i) \right] + \lambda \sum_{j=1}^p |\beta_j| \right\}
$$

where:

- $\hat{p}_i = \frac{1}{1 + e^{-X_i\beta}}$ is the predicted probability for the $i$-th sample.
- $y_i$ represents the actual class label (binary: 0 or 1).
- $X_i$ is the feature vector for the $i$-th observation.
- $\beta$ is the vector of model coefficients (including the intercept).
- $\lambda$ is the regularization parameter that controls the trade-off between model fit and sparsity (higher $\lambda$) encourages sparsity by shrinking more coefficients to zero).


The lasso penalty encourages the sum of the absolute values of the
coefficients to be small, effectively shrinking some coefficients to
zero. This results in sparser solutions, simplifying the model and
reducing variance without substantial increase in bias.


Practical benefits of LASSO:

- Dimensionality Reduction: LASSO is particularly useful when the
  number of features $p$ is large, potentially even larger than the
  number of observations $n$, as it automatically reduces the number
  of features.
- Preventing Overfitting: The L1 penalty helps prevent overfitting
  by constraining the model, especially when $p$ is large or there is
  multicollinearity among features.
- Interpretability: By selecting only the most important features,
  LASSO makes the resulting model more interpretable, which is
  valuable in fields like bioinformatics, economics, and social
  sciences.


### Solution Path

To illustrate the effect of the lasso penalty in logistic regression,
we can plot the solution path of the coefficients as a function of the
regularization parameter $\lambda$. This demonstration will use a
simulated dataset to show how increasing $\lambda$ leads to more
coefficients being set to zero.

```{python}
import numpy as np
from sklearn.datasets import make_classification
from sklearn.linear_model import LogisticRegression
from sklearn.preprocessing import StandardScaler
import matplotlib.pyplot as plt

# Step 1: Generate a classification dataset
X, y = make_classification(n_samples=100, n_features=20, n_informative=2,
                               random_state=42)

# Step 2: Get a lambda grid given length of lambda and min_ratio of lambda_max
def get_lambda_l1(xs: np.ndarray, y: np.ndarray, nlambda: int, min_ratio: float):
    ybar = np.mean(y)
    xbar = np.mean(xs, axis=0)
    xs_centered = xs - xbar
    xty = np.dot(xs_centered.T, (y - ybar))
    lmax = np.max(np.abs(xty))
    lambdas = np.logspace(np.log10(lmax), np.log10(min_ratio * lmax),
                              num=nlambda)
    return lambdas

# Step 3: Calculate lambda values
nlambda = 100
min_ratio = 0.01
lambda_values = get_lambda_l1(X, y, nlambda, min_ratio)

# Step 4: Standardize the features
scaler = StandardScaler()
X_scaled = scaler.fit_transform(X)

# Step 5: Initialize arrays to store the coefficients for each lambda value
coefficients = []

# Step 6: Fit logistic regression with L1 regularization (Lasso) for each lambda value
for lam in lambda_values:
    model = LogisticRegression(penalty='l1', solver='liblinear', C=1/lam, max_iter=1000)
    model.fit(X_scaled, y)
    coefficients.append(model.coef_.flatten())

# Convert coefficients list to a NumPy array for plotting
coefficients = np.array(coefficients)

# Step 7: Plot the solution path for each feature
plt.figure(figsize=(10, 6))
for i in range(coefficients.shape[1]):
    plt.plot(lambda_values, coefficients[:, i], label=f'Feature {i + 1}')
    
plt.xscale('log')
plt.xlabel('Lambda values (log scale)')
plt.ylabel('Coefficient value')
plt.title('Solution Path of Logistic Lasso Regression')
plt.grid(True)
plt.legend(bbox_to_anchor=(1.05, 1), loc='upper left')
plt.show()
```


### Selection the Tuning Parameter

In logistic regression with LASSO regularization, selecting the
optimal value of the regularization parameter $C$ (the inverse of
$\lambda$) is crucial to balancing the model's bias and
variance. A small $C$ value (large $\lambda$) increases the
regularization effect, shrinking more coefficients to zero and
simplifying the model. Conversely, a large $C$ (small $\lambda$)
allows the model to fit the data more closely.


The best way to select the optimal $C$ is through
cross-validation. In cross-validation, the dataset is split into
several folds, and the model is trained on some folds while evaluated
on the remaining fold. This process is repeated for each fold, and the
results are averaged to ensure the model generalizes well to unseen
data. The $C$ value that results in the best performance is
selected.


The performance metric used in cross-validation can vary based on the
task. Common metrics include:

- Log-loss: Measures how well the predicted probabilities match the actual outcomes.
- Accuracy: Measures the proportion of correctly classified instances.
- F1-Score: Balances precision and recall, especially useful for imbalanced classes.
- AUC-ROC: Evaluates how well the model discriminates between the positive and negative classes.


In Python, the `LogisticRegressionCV` class from `scikit-learn`
automates cross-validation for logistic regression. It evaluates the
model's performance for a range of $C$ values and selects the best
one.

```{python}
import numpy as np
from sklearn.linear_model import LogisticRegressionCV
from sklearn.model_selection import train_test_split
from sklearn.datasets import make_classification
from sklearn.metrics import accuracy_score

# Generate synthetic data
X, y = make_classification(n_samples=1000, n_features=20, random_state=42)

# Split the dataset into training and testing sets
X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.25, random_state=42)

# Initialize LogisticRegressionCV with L1 penalty for Lasso and cross-validation
log_reg_cv = LogisticRegressionCV(
    Cs=np.logspace(-4, 4, 20),  # Range of C values (inverse of lambda)
    cv=5,                       # 5-fold cross-validation
    penalty='l1',               # Lasso regularization (L1 penalty)
    solver='liblinear',         # Solver for L1 regularization
    scoring='accuracy',         # Optimize for accuracy
    max_iter=10000              # Ensure convergence
)

# Train the model with cross-validation
log_reg_cv.fit(X_train, y_train)

# Best C value (inverse of lambda)
print(f"Best C value: {log_reg_cv.C_[0]}")

# Evaluate the model on the test set
y_pred = log_reg_cv.predict(X_test)
test_accuracy = accuracy_score(y_test, y_pred)
print(f"Test Accuracy: {test_accuracy:.2f}")

# Display the coefficients of the best model
print("Model Coefficients:\n", log_reg_cv.coef_)
```


### Preparing for Logistic Regression Fitting

The `LogisticRegression()` function in `scikit.learn` takes the design
matrix of the regression as input, which needs to be prepared with
care from the covariates or features that we have. 


#### Continuous Variables
For continuous variables, it is often desirable to standardized them
so that they have mean zero and standard deviation one. There are
multiple advantages of doing so. It improves numerical stability in
algorithms like logistic regression that rely on gradient descent,
ensuring faster convergence and preventing features with large scales
from dominating the optimization process. Standardization also
enhances the interpretability of model coefficients by allowing for
direct comparison of the effects of different features, as
coefficients then represent the change in outcome for a one standard
deviation increase in each variable. Additionally, it ensures that
regularization techniques like Lasso and Ridge treat all features
equally, allowing the model to select the most relevant ones without
being biased by feature magnitude.


Moreover, standardization is essential for distance-based models such
as k-Nearest Neighbors (k-NN) and Support Vector Machines (SVMs),
where differences in feature scale can distort the calculations. It
also prevents models from being sensitive to arbitrary changes in the
units of measurement, improving robustness and consistency. Finally,
standardization facilitates better visualizations and diagnostics by
putting all variables on a comparable scale, making patterns and
residuals easier to interpret. Overall, it is a simple yet powerful
preprocessing step that leads to better model performance and
interpretability.


We have already seen this with `StandardScaler`.


#### Categorical Variables

Categorical variables can be classified into two types: nominal and
ordinal. Nominal variables represent categories with no inherent order
or ranking between them. Examples include variables like "gender"
(male, female) or "color" (red, blue, green), where the categories are
simply labels and one category does not carry more significance than
another. Ordinal variables, on the other hand, represent categories
with a meaningful order or ranking. For example, education levels such
as "high school," "bachelor," "master," and "PhD" have a clear
hierarchy, where each level is ranked higher than the previous
one. However, the differences between the ranks are not necessarily
uniform or quantifiable, making ordinal variables distinct from
numerical variables. Understanding the distinction between nominal and
ordinal variables is important when deciding how to encode and
interpret them in statistical models.


Categorical variables needs to be coded into numerical values before
further processing.  In Python, nominal and ordinal variables are
typically encoded differently to account for their unique
properties. Nominal variables, which have no inherent order, are often
encoded using One-Hot Encoding, where each category is transformed
into a binary column (0 or 1). For example, the OneHotEncoder from
scikit-learn can be used to convert a "color" variable with categories
like "red," "blue," and "green" into separate columns color_red,
color_blue, and color_green, with only one column being 1 for each
observation. On the other hand, ordinal variables, which have a
meaningful order, are best encoded using Ordinal Encoding. This method
assigns an integer to each category based on their rank. For example,
an "education" variable with categories "high school," "bachelor,"
"master," and "PhD" can be encoded as 0, 1, 2, and 3,
respectively. The OrdinalEncoder from scikit-learn can be used to
implement this encoding, which ensures that the model respects the
order of the categories during analysis.


#### An Example

Here is a demo with `pipeline` using a simulated dataset.

First we generate data with sample size 1000 from a logistic model
with both categorical and numerical covariates.
```{python}
import pandas as pd
from sklearn.model_selection import train_test_split
from sklearn.preprocessing import OneHotEncoder, StandardScaler
from sklearn.linear_model import LogisticRegression
from sklearn.pipeline import Pipeline
from sklearn.compose import ColumnTransformer
import numpy as np
from scipy.special import expit  # Sigmoid function

# Generate a dataset with the specified size
dataset_size = 1000
np.random.seed(20241014)

# Simulate categorical and numerical features
gender = np.random.choice(
    ['male', 'female'], size=dataset_size)  # Nominal variable
education = np.random.choice(
    ['high_school', 'bachelor', 'master', 'phd'], size=dataset_size)  # Ordinal variable
age = np.random.randint(18, 65, size=dataset_size)
income = np.random.randint(30000, 120000, size=dataset_size)

# Create a logistic relationship between the features and the outcome
gender_num = np.where(gender == 'male', 0, 1)

# Define the linear predictor with regression coefficients
linear_combination = (
    0.3 * gender_num - 0.02 * age + 0.00002 * income
)

# Apply sigmoid function to get probabilities
probabilities = expit(linear_combination)

# Generate binary outcome based on the probabilities
outcome = np.random.binomial(1, probabilities)

# Create a DataFrame
data = pd.DataFrame({
    'gender': gender,
    'education': education,
    'age': age,
    'income': income,
    'outcome': outcome
})
```

Next we split the data into features and target and define
transformers for each types of feature columns.

```{python}
# Split the dataset into features (X) and target (y)
X = data[['gender', 'education', 'age', 'income']]
y = data['outcome']

# Define categorical and numerical columns
categorical_cols = ['gender', 'education']  
numerical_cols = ['age', 'income']

# Define transformations for categorical variable
categorical_transformer = OneHotEncoder(
    categories=[['male', 'female'], ['high_school', 'bachelor', 'master', 'phd']],
    drop='first')

# Define transformations for continuous variables
numerical_transformer = StandardScaler()

# Use ColumnTransformer to transform the columns
preprocessor = ColumnTransformer(
    transformers=[
        ('cat', categorical_transformer, categorical_cols),
        ('num', numerical_transformer, numerical_cols)
    ]
)
```

Define a pipeline, which preprocess the data and then fits a logistic
model.
```{python}
pipeline = Pipeline(steps=[
    ('preprocessor', preprocessor),
    ('classifier', LogisticRegression(penalty='l1', solver='liblinear',
    max_iter=1000))
])

# Split the data into training and testing sets
X_train, X_test, y_train, y_test = train_test_split(
    X, y, test_size=0.2, random_state=2024)

# Fit the pipeline to the training data
pipeline.fit(X_train, y_train)
```

Check the coefficients of the fitted logistic regression model.
```{python}
model = pipeline.named_steps['classifier']
intercept = model.intercept_
coefficients = model.coef_

# Check the preprocessor's encoding
encoded_columns = pipeline.named_steps['preprocessor']\
.transformers_[0][1].get_feature_names_out(categorical_cols)

# Show intercept, coefficients, and encoded feature names
intercept, coefficients, list(encoded_columns)
```

Note that the encoded columns has one for gender and three for
education, with `male` and `high_school` as reference levels,
respectively. The reference level was determined when calling 
`oneHotEncoder()` with `drop = 'first'`. If `categories` were not
specified, the first level in alphabetical order would be dropped. 
With the default `drop = 'none'`, the estimated coefficients will have
two columns that are not estimable and were set to zero. Obviously, if
no level were dropped in forming the model matrix, the columns of the
one hot encoding for each categorical variable would be perfectly
linearly dependent because they would sum to one.


The regression coefficients returned by the logistic regression model
in this case should be interpreted on the standardized scale of the
numerical covariates (e.g., `age` and `income`). This is because we
applied standardization to the numerical features using StandardScaler
in the pipeline before fitting the model. For example, the coefficient
for age would reflect the change in the log-odds of the outcome for a
1 standard deviation increase in age, rather than a 1-unit increase in
years. The coefficients for the
one-hot encoded categorical variables (gender and education) are on
the original scale because one-hot encoding does not change the scale
of the variables. For instance, the coefficient for `gender_female` tells
us how much the log-odds of the outcome changes when the observation
is male versus the reference category (`male`).


## Count Data Modeling

Count data consists of non-negative integers representing event 
occurrences over a fixed unit of time or space. These data often exhibit 
skewness, overdispersion, and a prevalence of zeros, requiring 
specialized statistical models.
Count data is common in applied fields such as urban planning and 
environmental studies. This section introduces statistical models for 
count data, focusing on the Poisson and Negative Binomial (NB) 
distributions. Their probability mass functions (pmfs) are linked to 
Generalized Linear Model (GLM) parameters.

### Poisson Regression

The Poisson model is a member of the GLM. It assumes that the count
variable $Y$ follows a Poisson distribution:

$$
\Pr(Y = y) = \frac{\lambda^y e^{-\lambda}}{y!}, \quad y = 0,1,2,\dots
$$
where $\lambda$ is the expected count, linked to predictor variables 
through a log link function:

$$
\log \lambda = X^{\top} \beta.
$$

Here, $X represents the vector of covariates, and $\beta$ denotes the 
regression coefficients. The model assumes equidispersion (i.e., 
$E[Y] = \text{Var}(Y)$), which is often violated in practice.


- The coefficient $\beta_j$ represents the log change in the expected 
count per unit increase in $X_j$.
- Exponentiating $\beta_j$ provides the multiplicative effect on the 
mean count.
- If $\beta_j > 0$, increasing $X_j$ leads to higher counts, while 
$\beta_j < 0$ suggests a negative association

### Negative Binomial Regression

When overdispersion (variance exceeding the mean) is present, the 
Negative Binomial (NB) regression provides a more flexible alternative. 
The NB model introduces an overdispersion parameter $\theta$, modifying 
the variance structure:

$$
\Pr(Y = y) = \frac{\Gamma(y + \theta^{-1})}{y! \Gamma(\theta^{-1})} 
\left( \frac{\theta \lambda}{1 + \theta \lambda} \right)^y 
\left( \frac{1}{1 + \theta \lambda} \right)^{\theta^{-1}},
$$

where $\theta$ controls the degree of dispersion. The mean remains 
$\lambda$, but the variance expands to:

$$
\text{Var}(Y) = \lambda + \frac{\lambda^2}{\theta}.
$$

The log link function remains to be commonly used.


- Coefficients in NB regression are interpreted similarly to Poisson 
regression.
- The dispersion parameter $\theta$ quantifies the degree of 
overdispersion; larger values suggest the Poisson model may still be 
appropriate.

### Model Diagnosis

Assessing model fit is crucial in count data modeling. Common diagnostic 
methods include:

- Overdispersion Check: If the variance significantly exceeds the mean, 
NB regression is preferred.
- Goodness-of-Fit: Comparing Akaike Information Criterion (AIC) values 
for Poisson and NB models; lower AIC suggests a better fit.
- Residual Analysis: Examining Pearson and deviance residuals for 
systematic patterns.
- Zero-Inflation Check: If excess zeros exist, zero-inflated models may 
be required.


## An Example with NYC Street Flood

We use the NYC street flood data from the mid-term project. This
analysis focuses on two sewer-related complaints in 2024: Street
Flooding (SF) and Catch Basin (CB). SF complaints serve as a practical
indicator of street flooding, while CB complaints provide insights
into a key infrastructural factor---when catch basins fail to drain
rainwater properly due to blockages or structural issues, water
accumulates on the streets


Let's reformat the data to create count times series of SF and CB
complaints by zip code. 
```{python}
import pandas as pd
# Reload the dataset, ensuring 'Incident Zip' is read as a string from the start
df = pd.read_csv("data/nycflood2024.csv",
                 dtype={"Incident Zip": str}, parse_dates=["Created Date"])

# Filter for incidents in 2024, but also include 2023-12-31 for lag calculation
df_2024 = df[(df["Created Date"] >= "2023-12-31") &
                           (df["Created Date"] <= "2024-12-31")].copy()

# Extract date and ensure proper formatting
df_2024["Date"] = df_2024["Created Date"].dt.date
df_2024["Zipcode"] = df_2024["Incident Zip"].astype(str)

# Identify complaint types
df_2024["SFcount"] = df_2024["Descriptor"].\
str.contains("Street Flooding", na=False).astype(int)
df_2024["CBcount"] = df_2024["Descriptor"].\
str.contains("Catch Basin Clogged", na=False).astype(int)

# Aggregate counts by zip code and date
df_grouped = df_2024.groupby(["Zipcode", "Date"])[["SFcount", "CBcount"]].sum().reset_index()

# Generate a full range of dates including 2023-12-31 for lag calculation
all_dates = pd.date_range(start="2023-12-31", end="2024-12-31")
all_zipcodes = df_grouped["Zipcode"].unique()

# Create a complete grid of all zip codes and dates
multi_index = pd.MultiIndex.from_product([all_zipcodes, all_dates], names=["Zipcode", "Date"])
full_df = pd.DataFrame(index=multi_index).reset_index()

# Ensure 'Date' is in datetime format
full_df["Date"] = pd.to_datetime(full_df["Date"])
df_grouped["Date"] = pd.to_datetime(df_grouped["Date"])

# Merge to include all combinations and fill missing values with 0
df_final = full_df.merge(df_grouped, on=["Zipcode", "Date"], how="left").fillna(0)

# Convert counts to integers
df_final["SFcount"] = df_final["SFcount"].astype(int)
df_final["CBcount"] = df_final["CBcount"].astype(int)

# Add lag-1 variable for CBcount
df_final["CBcount_Lag1"] = df_final.groupby("Zipcode")["CBcount"].shift(1)

df_final.head()
```


Now let's fit a Poisson model.
```{python}
import statsmodels.api as sm
import statsmodels.formula.api as smf

# Filter out 2023-12-31 since it has missing values for lagged CBcount
df_model = df_final[df_final["Date"] >= "2024-01-01"].copy()

# Fit Poisson regression
poisson_model = smf.glm("SFcount ~ CBcount_Lag1", 
                         data=df_model, 
                         family=sm.families.Poisson()).fit()

poisson_model.summary()
```


Then we fit a NB model.
```{python}
# Fit Negative Binomial regression
nb_model = smf.negativebinomial("SFcount ~ CBcount_Lag1", 
                    data=df_model).fit()

nb_model.summary()
```
Note that `smf.glm` does not allow the dispersion parameter to be
estimated; instead, it is fixed at 1.


<!-- Check for overdisperson. -->
<!-- ```{python} -->
<!-- # Overdispersion check: Compute Pearson Chi-square statistic -->
<!-- poisson_deviance = poisson_model.pearson_chi2 / poisson_model.df_resid -->
<!-- nb_deviance = nb_model.pearson_chi2 / nb_model.df_resid -->

<!-- # Akaike Information Criterion (AIC) comparison -->
<!-- poisson_aic = poisson_model.aic -->
<!-- nb_aic = nb_model.aic -->

<!-- # Residual analysis -->
<!-- df_model["Poisson_Residuals"] = poisson_model.resid_pearson -->
<!-- df_model["NB_Residuals"] = nb_model.resid_pearson -->

<!-- # Summary of results -->
<!-- summary_results = { -->
<!--     "Poisson Coefficients": poisson_model.params.to_dict(), -->
<!--     "Negative Binomial Coefficients": nb_model.params.to_dict(), -->
<!--     "Poisson Deviance": poisson_deviance, -->
<!--     "Negative Binomial Deviance": nb_deviance, -->
<!--     "Poisson AIC": poisson_aic, -->
<!--     "Negative Binomial AIC": nb_aic -->
<!-- } -->

<!-- # Return the summary of findings -->
<!-- summary_results -->
<!-- ``` -->
