## Data Manipulation with Pandas

This section is prepared by Lang Lang. I am a senior student double majoring in  
data science and economics in University of Connecticut.

### Introduction

In this section, I will introduce about data manipulation using Pandas,  
which is a powerful Python library for working with data. I'll walk through  
some basic operations like filtering, merging, and summarizing data using a real  
data set of NYC motor vehicle collisions.

Pandas is a powerful Python library for data manipulation and analysis. It  
provides two key data structures:

-   Series: A one-dimensional labeled array.
-   Data Frame: A two-dimensional labeled table with rows and columns.

#### Why Use Pandas?

-   Efficiently handles large data sets.
-   Provides flexible data manipulation functions.
-   Works well with NumPy and visualization libraries like Matplotlib.

### Loading Data

#### Reading NYC Crash Data

We'll work with the NYC Motor Vehicle Collisions data set in the class notes  
repository.

-   Using the following code to import the Pandas library in Python

```{python}
import pandas as pd
```

-   Using the following code to load the data set

```{python}
df = pd.read_csv("data/nyccrashes_2024w0630_by20250212.csv")
```

#### Renaming The Columns

Using the following codes to rename the columns.

```{python}
df.columns = [col.strip().lower().replace(" ", "_") for col in df.columns]
df.columns
```

#### Viewing the First Few Rows

The head function in Pandas is used to display the first few rows of a DataFrame.

```{python}
df.head() # Default value of n is 5
```

#### Checking Dataset Structure

The info function in Pandas provides a summary of a DataFrame, including:

-   Number of rows and columns
-   Column names and data types
-   Number of non-null values per column
-   Memory usage

```{python}
df.info()
```

This tells us:

-   The dataset has 1876 rows and 29 columns.

-   Data types include float64(3), int64(9), object(17).

-   There are no missing values in any column.

-   The dataset consumes 425.2+ KB of memory.

#### Descriptive Statistics

The discribe function provides summary statistics for numerical columns in a  
Pandas DataFrame.

```{python}
df.describe()
```

This provides insights such as:

-   Total count of entries

-   Mean, min, and max values

-   Standard deviation

### Selecting and Filtering Data

#### Selecting Specific Columns

Sometimes, we only need certain columns.

In this case, you can use select function.

```{python}
df_selected = df[['crash_date', 'crash_time', 'borough', 
                  'number_of_persons_injured']]
df_selected.head() 
```

#### Filtering Data

when you would like to filter certain specific data (e.g., Crashes in 2024-06-30),  
you can using the following data to define:

```{python}
# Convert crash date to datetime format
df['crash_date'] = pd.to_datetime(df['crash_date'])

june30_df = df[df['crash_date'] == '2024-06-30']
june30_df.head()
```

#### Filter A DataFrame

The query function in Pandas is used to filter a DataFrame using a more  
readable, SQL-like syntax. For example, now I would like to find the crashes with  
the number of persons injured more than 2. We can using the following code:

```{python}
df_filtered = df.query("`number_of_persons_injured` > 2")
df_filtered.head()
```

### Merging DataFrames

In Pandas, the merge function is used to combine multiple DataFrames based  
on a common column.

This is useful when working with multiple datasets that need to be joined for  
analysis.

#### Example: Merging Crash Data with Total Injuries per Borough

Suppose we have a dataset containing crash details, and we want to analyze how  
the total number of injuries in each borough relates to individual crashes.

We can achieve this by aggregating the total injuries per borough and merging it  
with the crash dataset.

The following code:

-   Aggregates the total injuries per borough using `.groupby()`.

-   Selects relevant columns from the main dataset (`collision_id`, `borough`,  
    `number_of_persons_injured`).

-   Merges the aggregated injury data with the crash dataset using merge function on  
    the `borough` column.

```{python}
# Aggregate total injuries per borough
df_borough_info = df.groupby(
          'borough', as_index=False
          )['number_of_persons_injured'].sum()
df_borough_info.rename(
          columns={'number_of_persons_injured': 'total_injuries'}, 
                  inplace=True)

# Select relevant columns from the main dataset
df_crashes = df[['collision_id', 'borough', 'number_of_persons_injured']]

# Merge crash data with total injuries per borough
df_merged = pd.merge(df_crashes, df_borough_info, on='borough', how='left')

# Display first few rows of the merged dataset
df_merged.head()
```

The merged dataset now includes:

-   collision_id: Unique identifier for each crash.
-   borough: The borough where the crash occurred.
-   number_of_persons_injured: Number of injuries in a specific crash.
-   total_injuries: The total number of injuries reported in that borough.

This merged dataset allows us to compare individual crash injuries with the  
overall injury trend in each borough, helping in further data analysis and  
visualization.

### Data Visualization

Visualizing data is crucial to understanding patterns and relationships within  
the dataset. In this section, we will create one-variable tables (frequency  
table), two-variable tables (contingency table).

#### One-Variable Table

A one-variable table (also called a frequency table) shows the distribution of  
values for a single categorical variable.

For example, we can count the number of crashes per borough:

```{python}
borough_counts = df['borough'].value_counts()
borough_counts
```

This table displays the number of accidents recorded in each borough of NYC. It  
helps identify which borough has the highest accident frequency.

#### Two-Variable Table

A two-variable table (also called a contingency table) shows the relationship  
between two categorical variables.

For example, we can analyze the number of crashes per borough per day:

```{python}
# make pivot table
borough_day_table = df.pivot_table(
    index='crash_date',
    columns='borough',
    values='collision_id',
    aggfunc='count'
)
borough_day_table

```

This table shows the number of accidents per borough for each day in the dataset.

### Data Cleaning and Transformation

This part is from the textbook “Python for Data Analysis: Data Wrangling with  
Pandas, NumPy, and IPython.” Chapter 5, Third Edition by Wes McK- inney, O’Reilly  
Media, 2022. https://wesmckinney.com/book/.

#### Changing Data Types

The following functions in Pandas are used to convert data types. This is  
important when working with dates and categorical data to ensure proper analysis.

For example, we want to:

```{python}
# Convert crash date to datetime format
df['crash_date'] = pd.to_datetime(df['crash_date'])
df['crash_date']
  
# Convert ZIP code to string
df['zip_code'] = df['zip_code'].astype(str)
df['zip_code']
```

#### Sorting Data

We can sort data by one or more columns:

```{python}
# Sort by date and time
df_sorted = df.sort_values(
    by=['crash_date', 'crash_time'], 
    ascending=[True, True]
)
df_sorted.head()
```

Now you can see the data is sorted by crash time.

#### Handling Missing Data

Pandas provides methods for handling missing values.

For example, you can using the following codes to fix the missing data.

```{python}
# Check for missing values
df.isna().sum()
```

```{python}
# Fill missing values with NaN
df.fillna(float('nan'), inplace=True)
df.fillna
```

### Conclusion

Pandas is a powerful tool for data analysis. Learning how to use Pandas will  
allow you to perform more advanced analytics and become more proficent in using  
python.
